{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acecf7ce",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34129a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets pycocotools matplotlib\n",
    "!pip install lightning\n",
    "!pip install huggingface-hub==0.24.0\n",
    "!pip install datasets pycocotools matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e5cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/genereux/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token hf_xxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f06fea",
   "metadata": {},
   "source": [
    "### Import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01a654f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, ClassLabel\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import lightning as pl\n",
    "import torch\n",
    "from lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForObjectDetection\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c577c1",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdedc50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'val'])\n",
      "Total images in full dataset: 46781\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = load_dataset(\"detection-datasets/fashionpedia\")\n",
    "print(dataset_dict.keys())\n",
    "full_dataset = concatenate_datasets([dataset_dict[k] for k in dataset_dict])\n",
    "print(f\"Total images in full dataset: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46676ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target IDs: [13, 14, 15, 27]\n"
     ]
    }
   ],
   "source": [
    "label_names = dataset_dict[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "target_classes = {\n",
    "    \"glasses\",\n",
    "    \"hat\",\n",
    "    \"headband, head covering, hair accessory\",\n",
    "    \"hood\"\n",
    "}\n",
    "target_ids = [i for i, name in enumerate(label_names) if name.lower() in target_classes]\n",
    "id_to_name = {i: label_names[i] for i in target_ids}\n",
    "print(\"Target IDs:\", target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5c8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset_train = dataset_dict[\"train\"].filter(\n",
    "    lambda x: any(cat in target_ids for cat in x[\"objects\"][\"category\"])\n",
    ")\n",
    "filtered_dataset_val   = dataset_dict[\"val\"].filter(\n",
    "    lambda x: any(cat in target_ids for cat in x[\"objects\"][\"category\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e6621c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_target_objects(example):\n",
    "    filtered_boxes = []\n",
    "    filtered_labels = []\n",
    "    filtered_area = []\n",
    "    filtered_bbox_ids = []\n",
    "\n",
    "    for i, label in enumerate(example[\"objects\"][\"category\"]):\n",
    "        if label in target_ids:\n",
    "            filtered_labels.append(label)\n",
    "            filtered_boxes.append(example[\"objects\"][\"bbox\"][i])\n",
    "            filtered_area.append(example[\"objects\"][\"area\"][i])\n",
    "            filtered_bbox_ids.append(example[\"objects\"][\"bbox_id\"][i])\n",
    "\n",
    "    # Overwrite the \"objects\" field\n",
    "    example[\"objects\"] = {\n",
    "        \"category\": filtered_labels,\n",
    "        \"bbox\": filtered_boxes,\n",
    "        \"area\": filtered_area,\n",
    "        \"bbox_id\": filtered_bbox_ids,\n",
    "    }\n",
    "    return example\n",
    "\n",
    "dataset_train_cleaned = filtered_dataset_train.map(keep_only_target_objects)\n",
    "dataset_eval_cleaned  = filtered_dataset_val.map(keep_only_target_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa268f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "headwear_dataset  = DatasetDict({\n",
    "    \"train\": dataset_train_cleaned,\n",
    "    \"val\": dataset_eval_cleaned\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d4633",
   "metadata": {},
   "source": [
    "### Remap label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162b36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New 0-indexed label mapping: {13: 0, 14: 1, 15: 2, 27: 3}\n",
    "id_map = {orig_id: new_id for new_id, orig_id in enumerate(target_ids)}\n",
    "\n",
    "def remap_labels(example):\n",
    "    example[\"objects\"][\"category\"] = [\n",
    "        id_map[cat] for cat in example[\"objects\"][\"category\"] if cat in id_map\n",
    "    ]\n",
    "    return example\n",
    "headwear_dataset = headwear_dataset.map(remap_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8116f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset_dict[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "id_to_name = {new_id: label_names[old_id] for new_id, old_id in enumerate(target_ids)}\n",
    "new_class_names = [id_to_name[i] for i in range(len(id_to_name))]\n",
    "\n",
    "new_class_label = ClassLabel(num_classes=len(new_class_names), names=new_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d588e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy current feature schema\n",
    "new_features = deepcopy(headwear_dataset[\"train\"].features)\n",
    "\n",
    "# Overwrite the category class labels with your new ones\n",
    "new_features[\"objects\"].feature[\"category\"] = new_class_label\n",
    "\n",
    "# Cast both splits with new schema\n",
    "headwear_dataset[\"train\"] = headwear_dataset[\"train\"].cast(new_features)\n",
    "headwear_dataset[\"val\"] = headwear_dataset[\"val\"].cast(new_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab75e1b",
   "metadata": {},
   "source": [
    "* Quick view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58aa0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glasses', 'hat', 'headband, head covering, hair accessory', 'hood']\n"
     ]
    }
   ],
   "source": [
    "print(headwear_dataset[\"train\"].features[\"objects\"].feature[\"category\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04bf193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'glasses',\n",
       " 1: 'hat',\n",
       " 2: 'headband, head covering, hair accessory',\n",
       " 3: 'hood'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a41ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bbox_id': [158957],\n",
       " 'category': [0],\n",
       " 'bbox': [[284.0, 135.0, 372.0, 169.0]],\n",
       " 'area': [1468]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headwear_dataset['train'][0]['objects']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5c270",
   "metadata": {},
   "source": [
    "### Categories and Label Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "568f11cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10436/10436 [01:06<00:00, 156.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning on classes: ['glasses', 'hat', 'headband, head covering, hair accessory', 'hood']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cats = headwear_dataset[\"train\"].features['objects'].feature['category']\n",
    "label_names = cats.names\n",
    "\n",
    "# Filter to only classes actually present\n",
    "used_label_ids = set()\n",
    "for example in tqdm(headwear_dataset[\"train\"]):\n",
    "    used_label_ids.update(example[\"objects\"][\"category\"])\n",
    "\n",
    "included_label_ids = sorted(list(used_label_ids))\n",
    "included_label_names = [label_names[i] for i in included_label_ids]\n",
    "\n",
    "print(\"Fine-tuning on classes:\", included_label_names)\n",
    "\n",
    "def idx_to_text(indexes):\n",
    "    return [label_names[i] for i in indexes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446de880",
   "metadata": {},
   "source": [
    "### Load Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec99c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/models/yolos/feature_extraction_yolos.py:38: FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.\n",
      "  warnings.warn(\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"hustvl/yolos-small\", size=816, max_size=864)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85bd11",
   "metadata": {},
   "source": [
    "### Transform Function & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c3a6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_channels(t):\n",
    "    \"\"\"\n",
    "    Some images may have 4 channels (transparent images) or just 1 channel (black and white images), in order to let the images have only 3 channels. I am going to remove the fourth channel in transparent images and stack the single channel in back and white images.\n",
    "    :param t: Tensor-like image\n",
    "    :return: Tensor-like image with three channels\n",
    "    \"\"\"\n",
    "    if len(t.shape) == 2:\n",
    "        return ToPILImage()(torch.stack([t for _ in range(3)]))\n",
    "    if t.shape[0] == 4:\n",
    "        return ToPILImage()(t[:3])\n",
    "    if t.shape[0] == 1:\n",
    "        return ToPILImage()(torch.stack([t[0]] * 3))\n",
    "    return ToPILImage()(t)\n",
    "\n",
    "def xyxy_to_xcycwh(box):\n",
    "    \"\"\"\n",
    "    Boxes in images may have the format (x1, y1, x2, y2) and we may need the format (center of x, center of y, width, height).\n",
    "    :param box: Tensor-like box with format (x1, y1, x2, y2)\n",
    "    :return: Tensor-like box with format (center of x, center of y, width, height)\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box.unbind(dim=1)\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    xc, yc = x1 + w * 0.5, y1 + h * 0.5\n",
    "    return torch.stack([xc, yc, w, h], dim=1)\n",
    "\n",
    "def rescale_bboxes(bboxes, size, down=True):\n",
    "    \"\"\"\n",
    "    Boxes information contains values between 0 and 1 instead of values in pixels. This is made in order to make the boxes independant from the size of the image. \n",
    "    But we may need to re-escale the box.\n",
    "    \"\"\"\n",
    "    w, h = size\n",
    "    scale = torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "    return (torch.tensor(bboxes) / scale if down else torch.tensor(bboxes) * scale)\n",
    "\n",
    "def transform(batch):\n",
    "    inputs = {}\n",
    "    image = batch['image']\n",
    "    image = fix_channels(ToTensor()(image[0]))\n",
    "    inputs['pixel_values'] = feature_extractor([image], return_tensors='pt')['pixel_values']\n",
    "    labels = []\n",
    "    bbox = [rescale_bboxes(batch['objects'][i]['bbox'], (batch['width'][i], batch['height'][i])) for i in range(len(batch['objects']))]\n",
    "    bbox = [xyxy_to_xcycwh(torch.Tensor(bbox_i)) for bbox_i in bbox]\n",
    "    labels.append({\n",
    "        \"boxes\": bbox,\n",
    "        \"class_labels\": [object['category'] for object in batch['objects']],\n",
    "        \"image_id\": torch.Tensor([batch['image_id']]).int(),\n",
    "        \"area\": [object['area'] for object in batch['objects']],\n",
    "        \"iscrowd\": torch.Tensor([0 for _ in batch['objects']]).int(),\n",
    "        \"orig_size\": torch.Tensor([(batch['width'], batch['height'])]).int(),\n",
    "        \"size\": torch.Tensor([inputs['pixel_values'].shape[1:]])[0].int(),\n",
    "    })\n",
    "    inputs['labels'] = labels\n",
    "    return inputs\n",
    "\n",
    "def plot_results(pil_img, boxes, class_labels):\n",
    "    COLORS = ['lightcoral', 'yellowgreen', 'darkturquoise', 'hotpink', 'mediumslateblue']\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for (xmin, ymin, xmax, ymax), c, label in zip(boxes, colors, class_labels):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=2))\n",
    "        ax.text(xmin, ymin, str(label), fontsize=15,\n",
    "                bbox=dict(facecolor=c, alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde23eeb",
   "metadata": {},
   "source": [
    "### Apply transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfd3f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train = headwear_dataset[\"train\"].with_transform(transform)\n",
    "prepared_val   = headwear_dataset[\"val\"].with_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f78d9",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    collated = {}\n",
    "    collated[\"pixel_values\"] = feature_extractor.pad([item['pixel_values'] for item in batch], return_tensors=\"pt\")['pixel_values']\n",
    "    collated[\"labels\"] = []\n",
    "    for item in batch:\n",
    "        item['labels']['boxes'] = torch.stack(item['labels']['boxes'])[0]\n",
    "        item['labels']['area'] = torch.Tensor(item['labels']['area'])\n",
    "        item['labels']['class_labels'] = torch.Tensor(item['labels']['class_labels'])[0]\n",
    "        item['labels']['class_labels'] = item['labels']['class_labels'].type(torch.LongTensor)\n",
    "        collated[\"labels\"].append(item['labels'])\n",
    "    return collated\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "train_dataloader = DataLoader(prepared_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(prepared_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2073f",
   "metadata": {},
   "source": [
    "### Define Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ccbe149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "    def __init__(self, lr=2.5e-5, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForObjectDetection.from_pretrained(\n",
    "            \"hustvl/yolos-small\",\n",
    "            num_labels=len(target_ids),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        return self.model(pixel_values=pixel_values)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "        self.log(\"train_loss\", outputs.loss)\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "        self.log(\"val_loss\", outputs.loss)\n",
    "        return outputs.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953af60",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0e1cfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of YolosForObjectDetection were not initialized from the model checkpoint at hustvl/yolos-small and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.layers.2.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- class_labels_classifier.layers.2.weight: found shape torch.Size([92, 384]) in the checkpoint and torch.Size([5, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                    | Params | Mode\n",
      "---------------------------------------------------------\n",
      "0 | model | YolosForObjectDetection | 30.7 M | eval\n",
      "---------------------------------------------------------\n",
      "30.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.7 M    Total params\n",
      "122.605   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "237       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a4d5a491454bb28c6c1f2b968a9112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Detr()\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#mps, cpu, cuda\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:138\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m     l \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Keep as-is per image\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat(),  \u001b[38;5;66;03m# Tensor (N, 4)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlong(),  \u001b[38;5;66;03m# Tensor (N,)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m: l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m: l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morig_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morig_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m     })\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: pixel_values,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels\n\u001b[1;32m     33\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "model = Detr()\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"mps\", devices=1) #mps, cpu, cuda\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e42dfb",
   "metadata": {},
   "source": [
    "### Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf33d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.push_to_hub(\"Genereux-akotenou/yolos-headwear\", private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501abad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
