{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acecf7ce",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34129a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets Pillow\n",
    "!pip install pycocotools matplotlib\n",
    "!pip install lightning\n",
    "!pip install huggingface-hub==0.24.0\n",
    "!pip install datasets pycocotools matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f06fea",
   "metadata": {},
   "source": [
    "### Import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, ClassLabel\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import lightning as pl\n",
    "import torch\n",
    "from lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForObjectDetection\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c577c1",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(\n",
    "    \"detection-datasets/fashionpedia\",\n",
    "    cache_dir=\"./hf_cache\"\n",
    ")\n",
    "print(dataset_dict.keys())\n",
    "full_dataset = concatenate_datasets([dataset_dict[k] for k in dataset_dict])\n",
    "print(f\"Total images in full dataset: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset_dict[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "target_classes = {\n",
    "    \"glasses\",\n",
    "    \"hat\",\n",
    "    \"headband, head covering, hair accessory\",\n",
    "    \"hood\"\n",
    "}\n",
    "target_ids = [i for i, name in enumerate(label_names) if name.lower() in target_classes]\n",
    "id_to_name = {i: label_names[i] for i in target_ids}\n",
    "print(\"Target IDs:\", target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset_train = dataset_dict[\"train\"].filter(\n",
    "    lambda x: any(cat in target_ids for cat in x[\"objects\"][\"category\"])\n",
    ")\n",
    "filtered_dataset_val   = dataset_dict[\"val\"].filter(\n",
    "    lambda x: any(cat in target_ids for cat in x[\"objects\"][\"category\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6621c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_target_objects(example):\n",
    "    filtered_boxes = []\n",
    "    filtered_labels = []\n",
    "    filtered_area = []\n",
    "    filtered_bbox_ids = []\n",
    "\n",
    "    for i, label in enumerate(example[\"objects\"][\"category\"]):\n",
    "        if label in target_ids:\n",
    "            filtered_labels.append(label)\n",
    "            filtered_boxes.append(example[\"objects\"][\"bbox\"][i])\n",
    "            filtered_area.append(example[\"objects\"][\"area\"][i])\n",
    "            filtered_bbox_ids.append(example[\"objects\"][\"bbox_id\"][i])\n",
    "\n",
    "    # Overwrite the \"objects\" field\n",
    "    example[\"objects\"] = {\n",
    "        \"category\": filtered_labels,\n",
    "        \"bbox\": filtered_boxes,\n",
    "        \"area\": filtered_area,\n",
    "        \"bbox_id\": filtered_bbox_ids,\n",
    "    }\n",
    "    return example\n",
    "\n",
    "dataset_train_cleaned = filtered_dataset_train.map(keep_only_target_objects)\n",
    "dataset_eval_cleaned  = filtered_dataset_val.map(keep_only_target_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa268f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "headwear_dataset  = DatasetDict({\n",
    "    \"train\": dataset_train_cleaned,\n",
    "    \"val\": dataset_eval_cleaned\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d4633",
   "metadata": {},
   "source": [
    "### Remap label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New 0-indexed label mapping: {13: 0, 14: 1, 15: 2, 27: 3}\n",
    "id_map = {orig_id: new_id for new_id, orig_id in enumerate(target_ids)}\n",
    "\n",
    "def remap_labels(example):\n",
    "    example[\"objects\"][\"category\"] = [\n",
    "        id_map[cat] for cat in example[\"objects\"][\"category\"] if cat in id_map\n",
    "    ]\n",
    "    return example\n",
    "headwear_dataset = headwear_dataset.map(remap_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8116f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset_dict[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "id_to_name = {new_id: label_names[old_id] for new_id, old_id in enumerate(target_ids)}\n",
    "new_class_names = [id_to_name[i] for i in range(len(id_to_name))]\n",
    "\n",
    "new_class_label = ClassLabel(num_classes=len(new_class_names), names=new_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d588e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy current feature schema\n",
    "new_features = deepcopy(headwear_dataset[\"train\"].features)\n",
    "\n",
    "# Overwrite the category class labels with your new ones\n",
    "new_features[\"objects\"].feature[\"category\"] = new_class_label\n",
    "\n",
    "# Cast both splits with new schema\n",
    "headwear_dataset[\"train\"] = headwear_dataset[\"train\"].cast(new_features)\n",
    "headwear_dataset[\"val\"] = headwear_dataset[\"val\"].cast(new_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab75e1b",
   "metadata": {},
   "source": [
    "* Quick view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headwear_dataset[\"train\"].features[\"objects\"].feature[\"category\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a41ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headwear_dataset['train'][0]['objects']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5c270",
   "metadata": {},
   "source": [
    "### Categories and Label Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = headwear_dataset[\"train\"].features['objects'].feature['category']\n",
    "label_names = cats.names\n",
    "\n",
    "# Filter to only classes actually present\n",
    "used_label_ids = set()\n",
    "for example in tqdm(headwear_dataset[\"train\"]):\n",
    "    used_label_ids.update(example[\"objects\"][\"category\"])\n",
    "\n",
    "included_label_ids = sorted(list(used_label_ids))\n",
    "included_label_names = [label_names[i] for i in included_label_ids]\n",
    "\n",
    "print(\"Fine-tuning on classes:\", included_label_names)\n",
    "\n",
    "def idx_to_text(indexes):\n",
    "    return [label_names[i] for i in indexes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446de880",
   "metadata": {},
   "source": [
    "### Load Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"hustvl/yolos-small\", size=816, max_size=864)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85bd11",
   "metadata": {},
   "source": [
    "### Transform Function & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_channels(t):\n",
    "    \"\"\"\n",
    "    Some images may have 4 channels (transparent images) or just 1 channel (black and white images), in order to let the images have only 3 channels. I am going to remove the fourth channel in transparent images and stack the single channel in back and white images.\n",
    "    :param t: Tensor-like image\n",
    "    :return: Tensor-like image with three channels\n",
    "    \"\"\"\n",
    "    if len(t.shape) == 2:\n",
    "        return ToPILImage()(torch.stack([t for _ in range(3)]))\n",
    "    if t.shape[0] == 4:\n",
    "        return ToPILImage()(t[:3])\n",
    "    if t.shape[0] == 1:\n",
    "        return ToPILImage()(torch.stack([t[0]] * 3))\n",
    "    return ToPILImage()(t)\n",
    "\n",
    "def xyxy_to_xcycwh(box):\n",
    "    \"\"\"\n",
    "    Boxes in images may have the format (x1, y1, x2, y2) and we may need the format (center of x, center of y, width, height).\n",
    "    :param box: Tensor-like box with format (x1, y1, x2, y2)\n",
    "    :return: Tensor-like box with format (center of x, center of y, width, height)\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box.unbind(dim=1)\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    xc, yc = x1 + w * 0.5, y1 + h * 0.5\n",
    "    return torch.stack([xc, yc, w, h], dim=1)\n",
    "\n",
    "def rescale_bboxes(bboxes, size, down=True):\n",
    "    \"\"\"\n",
    "    Boxes information contains values between 0 and 1 instead of values in pixels. This is made in order to make the boxes independant from the size of the image. \n",
    "    But we may need to re-escale the box.\n",
    "    \"\"\"\n",
    "    w, h = size\n",
    "    scale = torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "    return (torch.tensor(bboxes) / scale if down else torch.tensor(bboxes) * scale)\n",
    "\n",
    "def transform(batch):\n",
    "    inputs = {}\n",
    "    image = batch['image']\n",
    "    image = fix_channels(ToTensor()(image[0]))\n",
    "    inputs['pixel_values'] = feature_extractor([image], return_tensors='pt')['pixel_values']\n",
    "    labels = []\n",
    "    bbox = [rescale_bboxes(batch['objects'][i]['bbox'], (batch['width'][i], batch['height'][i])) for i in range(len(batch['objects']))]\n",
    "    bbox = [xyxy_to_xcycwh(torch.Tensor(bbox_i)) for bbox_i in bbox]\n",
    "    labels.append({\n",
    "        \"boxes\": bbox,\n",
    "        \"class_labels\": [object['category'] for object in batch['objects']],\n",
    "        \"image_id\": torch.Tensor([batch['image_id']]).int(),\n",
    "        \"area\": [object['area'] for object in batch['objects']],\n",
    "        \"iscrowd\": torch.Tensor([0 for _ in batch['objects']]).int(),\n",
    "        \"orig_size\": torch.Tensor([(batch['width'], batch['height'])]).int(),\n",
    "        \"size\": torch.Tensor([inputs['pixel_values'].shape[1:]])[0].int(),\n",
    "    })\n",
    "    inputs['labels'] = labels\n",
    "    return inputs\n",
    "\n",
    "def plot_results(pil_img, boxes, class_labels):\n",
    "    COLORS = ['lightcoral', 'yellowgreen', 'darkturquoise', 'hotpink', 'mediumslateblue']\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for (xmin, ymin, xmax, ymax), c, label in zip(boxes, colors, class_labels):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=2))\n",
    "        ax.text(xmin, ymin, str(label), fontsize=15,\n",
    "                bbox=dict(facecolor=c, alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde23eeb",
   "metadata": {},
   "source": [
    "### Apply transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train = headwear_dataset[\"train\"].with_transform(transform)\n",
    "prepared_val   = headwear_dataset[\"val\"].with_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f78d9",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    collated = {}\n",
    "    collated[\"pixel_values\"] = feature_extractor.pad([item['pixel_values'] for item in batch], return_tensors=\"pt\")['pixel_values']\n",
    "    collated[\"labels\"] = []\n",
    "    for item in batch:\n",
    "        item['labels']['boxes'] = torch.stack(item['labels']['boxes'])[0]\n",
    "        item['labels']['area'] = torch.Tensor(item['labels']['area'])\n",
    "        item['labels']['class_labels'] = torch.Tensor(item['labels']['class_labels'])[0]\n",
    "        item['labels']['class_labels'] = item['labels']['class_labels'].type(torch.LongTensor)\n",
    "        collated[\"labels\"].append(item['labels'])\n",
    "    return collated\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "train_dataloader = DataLoader(prepared_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(prepared_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2073f",
   "metadata": {},
   "source": [
    "### Define Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccbe149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "    def __init__(self, lr=2.5e-5, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForObjectDetection.from_pretrained(\n",
    "            \"hustvl/yolos-small\",\n",
    "            num_labels=len(target_ids),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        return self.model(pixel_values=pixel_values)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "        self.log(\"train_loss\", outputs.loss)\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "        self.log(\"val_loss\", outputs.loss)\n",
    "        return outputs.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953af60",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(save_dir=\"./logs-metrics\", name=\"yolos_run\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    "    filename=\"best-checkpoint\"\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detr()\n",
    "trainer = Trainer(\n",
    "    max_epochs=10, \n",
    "    accelerator=\"gpu\",  # mps, cpu, gpu\n",
    "    devices=1, \n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e42dfb",
   "metadata": {},
   "source": [
    "### Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf33d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config.id2label = id_to_name\n",
    "model.model.config.label2id = {v: k for k, v in id_to_name.items()}\n",
    "model.model.push_to_hub(\"Genereux-akotenou/yolos-headwear-2\", private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501abad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
